{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "read_geolife.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP1C5iEcnLk4X78FXSk4KY8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/snpushpi/Covid19_Contact_tracing_repo_beta/blob/master/read_geolife.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAKHOUXILl8u",
        "colab_type": "text"
      },
      "source": [
        "This code has been used for reading the geolife plt files and converting them to pandas dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYFLvWugLrmw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os.path\n",
        "import datetime\n",
        "import os\n",
        "\n",
        "def read_plt(plt_file):\n",
        "    points = pd.read_csv(plt_file, skiprows=6, header=None,\n",
        "                         parse_dates=[[5, 6]], infer_datetime_format=True)\n",
        "\n",
        "    # for clarity rename columns\n",
        "    points.rename(inplace=True, columns={'5_6': 'time', 0: 'lat', 1: 'lon', 3: 'alt'})\n",
        "\n",
        "    # remove unused columns\n",
        "    points.drop(inplace=True, columns=[2, 4])\n",
        "\n",
        "    return points\n",
        "\n",
        "mode_names = ['walk', 'bike', 'bus', 'car', 'subway','train', 'airplane', 'boat', 'run', 'motorcycle', 'taxi']\n",
        "mode_ids = {s : i + 1 for i, s in enumerate(mode_names)}\n",
        "\n",
        "def read_labels(labels_file):\n",
        "    labels = pd.read_csv(labels_file, skiprows=1, header=None,\n",
        "                         parse_dates=[[0, 1], [2, 3]],\n",
        "                         infer_datetime_format=True, delim_whitespace=True)\n",
        "\n",
        "    # for clarity rename columns\n",
        "    labels.columns = ['start_time', 'end_time', 'label']\n",
        "\n",
        "    # replace 'label' column with integer encoding\n",
        "    labels['label'] = [mode_ids[i] for i in labels['label']]\n",
        "\n",
        "    return labels\n",
        "\n",
        "def apply_labels(points, labels):\n",
        "    indices = labels['start_time'].searchsorted(points['time'], side='right') - 1\n",
        "    no_label = (indices < 0) | (points['time'].values >= labels['end_time'].iloc[indices].values)\n",
        "    points['label'] = labels['label'].iloc[indices].values\n",
        "    points['label'][no_label] = 0\n",
        "\n",
        "def read_user(user_folder):\n",
        "    labels = None\n",
        "\n",
        "    plt_files = glob.glob(os.path.join(user_folder, 'Trajectory', '*.plt'))\n",
        "    df = pd.concat([read_plt(f) for f in plt_files])\n",
        "\n",
        "    labels_file = os.path.join(user_folder, 'labels.txt')\n",
        "    if os.path.exists(labels_file):\n",
        "        labels = read_labels(labels_file)\n",
        "        apply_labels(df, labels)\n",
        "    else:\n",
        "        df['label'] = 0\n",
        "\n",
        "    return df\n",
        "\n",
        "def read_all_users(folder):\n",
        "    subfolders = os.listdir(folder)\n",
        "    dfs = []\n",
        "    for i, sf in enumerate(subfolders):\n",
        "        print('[%d/%d] processing user %s' % (i + 1, len(subfolders), sf))\n",
        "        df = read_user(os.path.join(folder,sf))\n",
        "        df['user'] = int(sf)\n",
        "        dfs.append(df)\n",
        "    return pd.concat(dfs)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}